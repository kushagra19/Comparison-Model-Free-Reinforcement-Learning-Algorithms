{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff69945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626c8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up agent and environment\n",
    "class agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"MountainCar-v0\")\n",
    "        self.env._max_episode_steps = 1000\n",
    "        self.pos_space = np.linspace(-1.2,0.6,20)\n",
    "        self.vel_space = np.linspace(-.07,.07,20)\n",
    "        self.action_space = [0,1,2]\n",
    "        self.state_space = []\n",
    "        self.Q = {}\n",
    "        self.returns = {}\n",
    "        self.visited = {}\n",
    "        self.policy = {}\n",
    "        \n",
    "        for pos_bin in range(len(self.pos_space)+1):\n",
    "            for vel_bin in range(len(self.vel_space)+1):\n",
    "                for action in self.action_space:\n",
    "                    self.Q[((pos_bin,vel_bin),action)] = 0\n",
    "                    self.returns[((pos_bin,vel_bin),action)] = 0\n",
    "                    self.visited[((pos_bin,vel_bin),action)] = 0\n",
    "                self.state_space.append((pos_bin,vel_bin))\n",
    "        \n",
    "        for state in self.state_space:\n",
    "            self.policy[state] = np.random.choice(self.action_space)\n",
    "                \n",
    "    def get_state(self,observation):\n",
    "        pos,vel = observation\n",
    "        pos_bin = np.digitize(pos,self.pos_space)\n",
    "        vel_bin = np.digitize(vel,self.vel_space)\n",
    "        \n",
    "        return pos_bin,vel_bin\n",
    "    \n",
    "    def max_action(self,state):\n",
    "        values = np.array([self.Q[state,a] for a in self.action_space])\n",
    "        action = np.argmax(values)\n",
    "        return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84877332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "agent = agent()\n",
    "num_episodes = 100000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "eps = 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e548881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode  1000 score  -1000.0 eps  0.95999999999996\n",
      "starting episode  2000 score  -1000.0 eps  0.91999999999992\n",
      "starting episode  3000 score  -1000.0 eps  0.87999999999988\n",
      "starting episode  4000 score  -1000.0 eps  0.83999999999984\n",
      "starting episode  5000 score  -486.0 eps  0.7999999999998\n",
      "starting episode  6000 score  -809.0 eps  0.75999999999976\n",
      "starting episode  7000 score  -437.0 eps  0.71999999999972\n",
      "starting episode  8000 score  -674.0 eps  0.67999999999968\n",
      "starting episode  9000 score  -497.0 eps  0.63999999999964\n",
      "starting episode  10000 score  -525.0 eps  0.5999999999996\n",
      "starting episode  11000 score  -242.0 eps  0.55999999999956\n",
      "starting episode  12000 score  -346.0 eps  0.51999999999952\n",
      "starting episode  13000 score  -436.0 eps  0.47999999999950776\n",
      "starting episode  14000 score  -247.0 eps  0.4399999999995233\n",
      "starting episode  15000 score  -197.0 eps  0.3999999999995388\n",
      "starting episode  16000 score  -322.0 eps  0.3599999999995543\n",
      "starting episode  17000 score  -250.0 eps  0.3199999999995698\n",
      "starting episode  18000 score  -186.0 eps  0.2799999999995853\n",
      "starting episode  19000 score  -140.0 eps  0.23999999999959384\n",
      "starting episode  20000 score  -159.0 eps  0.1999999999995816\n",
      "starting episode  21000 score  -192.0 eps  0.15999999999956935\n",
      "starting episode  22000 score  -158.0 eps  0.11999999999955885\n",
      "starting episode  23000 score  -144.0 eps  0.07999999999956048\n",
      "starting episode  24000 score  -249.0 eps  0.03999999999956211\n",
      "starting episode  25000 score  -217.0 eps  0.01\n",
      "starting episode  26000 score  -121.0 eps  0.01\n",
      "starting episode  27000 score  -190.0 eps  0.01\n",
      "starting episode  28000 score  -159.0 eps  0.01\n",
      "starting episode  29000 score  -154.0 eps  0.01\n",
      "starting episode  30000 score  -151.0 eps  0.01\n",
      "starting episode  31000 score  -207.0 eps  0.01\n",
      "starting episode  32000 score  -134.0 eps  0.01\n",
      "starting episode  33000 score  -131.0 eps  0.01\n",
      "starting episode  34000 score  -185.0 eps  0.01\n",
      "starting episode  35000 score  -129.0 eps  0.01\n",
      "starting episode  36000 score  -189.0 eps  0.01\n",
      "starting episode  37000 score  -159.0 eps  0.01\n",
      "starting episode  38000 score  -128.0 eps  0.01\n",
      "starting episode  39000 score  -136.0 eps  0.01\n",
      "starting episode  40000 score  -143.0 eps  0.01\n",
      "starting episode  41000 score  -223.0 eps  0.01\n",
      "starting episode  42000 score  -129.0 eps  0.01\n",
      "starting episode  43000 score  -134.0 eps  0.01\n",
      "starting episode  44000 score  -137.0 eps  0.01\n",
      "starting episode  45000 score  -145.0 eps  0.01\n",
      "starting episode  46000 score  -139.0 eps  0.01\n",
      "starting episode  47000 score  -107.0 eps  0.01\n",
      "starting episode  48000 score  -142.0 eps  0.01\n",
      "starting episode  49000 score  -125.0 eps  0.01\n",
      "starting episode  50000 score  -130.0 eps  0.01\n",
      "starting episode  51000 score  -132.0 eps  0.01\n",
      "starting episode  52000 score  -131.0 eps  0.01\n",
      "starting episode  53000 score  -150.0 eps  0.01\n",
      "starting episode  54000 score  -139.0 eps  0.01\n",
      "starting episode  55000 score  -142.0 eps  0.01\n",
      "starting episode  56000 score  -143.0 eps  0.01\n",
      "starting episode  57000 score  -141.0 eps  0.01\n",
      "starting episode  58000 score  -142.0 eps  0.01\n",
      "starting episode  59000 score  -139.0 eps  0.01\n",
      "starting episode  60000 score  -115.0 eps  0.01\n",
      "starting episode  61000 score  -132.0 eps  0.01\n",
      "starting episode  62000 score  -136.0 eps  0.01\n",
      "starting episode  63000 score  -133.0 eps  0.01\n",
      "starting episode  64000 score  -233.0 eps  0.01\n",
      "starting episode  65000 score  -131.0 eps  0.01\n",
      "starting episode  66000 score  -129.0 eps  0.01\n",
      "starting episode  67000 score  -147.0 eps  0.01\n",
      "starting episode  68000 score  -127.0 eps  0.01\n",
      "starting episode  69000 score  -131.0 eps  0.01\n",
      "starting episode  70000 score  -126.0 eps  0.01\n",
      "starting episode  71000 score  -167.0 eps  0.01\n",
      "starting episode  72000 score  -109.0 eps  0.01\n",
      "starting episode  73000 score  -109.0 eps  0.01\n",
      "starting episode  74000 score  -143.0 eps  0.01\n",
      "starting episode  75000 score  -148.0 eps  0.01\n",
      "starting episode  76000 score  -134.0 eps  0.01\n",
      "starting episode  77000 score  -138.0 eps  0.01\n",
      "starting episode  78000 score  -251.0 eps  0.01\n",
      "starting episode  79000 score  -145.0 eps  0.01\n",
      "starting episode  80000 score  -132.0 eps  0.01\n",
      "starting episode  81000 score  -139.0 eps  0.01\n",
      "starting episode  82000 score  -140.0 eps  0.01\n",
      "starting episode  83000 score  -156.0 eps  0.01\n",
      "starting episode  84000 score  -142.0 eps  0.01\n",
      "starting episode  85000 score  -146.0 eps  0.01\n",
      "starting episode  86000 score  -480.0 eps  0.01\n",
      "starting episode  87000 score  -132.0 eps  0.01\n",
      "starting episode  88000 score  -136.0 eps  0.01\n",
      "starting episode  89000 score  -134.0 eps  0.01\n",
      "starting episode  90000 score  -110.0 eps  0.01\n",
      "starting episode  91000 score  -236.0 eps  0.01\n",
      "starting episode  92000 score  -116.0 eps  0.01\n",
      "starting episode  93000 score  -147.0 eps  0.01\n",
      "starting episode  94000 score  -142.0 eps  0.01\n",
      "starting episode  95000 score  -142.0 eps  0.01\n",
      "starting episode  96000 score  -146.0 eps  0.01\n",
      "starting episode  97000 score  -149.0 eps  0.01\n",
      "starting episode  98000 score  -157.0 eps  0.01\n",
      "starting episode  99000 score  -149.0 eps  0.01\n"
     ]
    }
   ],
   "source": [
    "total_reward = np.zeros(num_episodes)\n",
    "#Algorithm starts\n",
    "for i in range(num_episodes):\n",
    "    obs = agent.env.reset()\n",
    "    done = False\n",
    "    if i%1000 == 0 and i>0:\n",
    "        print(\"starting episode \",i,\"score \",score,\"eps \",eps)\n",
    "        file = open(\"rewards_sarsa\",'wb')\n",
    "        pickle.dump(total_reward,file)\n",
    "        file.close()\n",
    "\n",
    "        # file = open(\"policy_sarsa\",'wb')\n",
    "        # pickle.dump(agent.policy,file)\n",
    "        # file.close()\n",
    "\n",
    "        file = open(\"Q_sarsa\",'wb')\n",
    "        pickle.dump(agent.Q,file)\n",
    "        file.close()\n",
    "    score = 0\n",
    "    state = agent.get_state(obs)\n",
    "    action = agent.max_action(state) if np.random.random() > eps \\\n",
    "                                    else agent.env.action_space.sample()\n",
    "    while not done:\n",
    "        obs_,reward,done,info = agent.env.step(action)\n",
    "        state_ = agent.get_state(obs_)\n",
    "        action_ = agent.max_action(state_) if np.random.random() > eps \\\n",
    "                                    else agent.env.action_space.sample()\n",
    "        score += reward\n",
    "        agent.Q[(state,action)] += alpha*(reward+gamma*agent.Q[(state_,action_)]-agent.Q[(state,action)])\n",
    "        state = state_\n",
    "        action = action_\n",
    "    total_reward[i] = score\n",
    "    eps = eps-4/num_episodes if eps > 0.01 else 0.01\n",
    "#     if eps - 1e-7 > 0:\n",
    "#         eps -= 1e-7\n",
    "#     else:\n",
    "#         eps = 0    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7967e9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be24304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
