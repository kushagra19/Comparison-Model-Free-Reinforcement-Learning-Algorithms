{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f6b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1203a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up agent and environment\n",
    "class agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"MountainCar-v0\")\n",
    "        self.env._max_episode_steps = 1000\n",
    "        self.pos_space = np.linspace(-1.2,0.6,20)\n",
    "        self.vel_space = np.linspace(-.07,.07,20)\n",
    "        self.action_space = [0,1,2]\n",
    "        self.state_space = []\n",
    "        self.Q = {}\n",
    "        self.returns = {}\n",
    "        self.visited = {}\n",
    "        self.policy = {}\n",
    "        \n",
    "        for pos_bin in range(len(self.pos_space)+1):\n",
    "            for vel_bin in range(len(self.vel_space)+1):\n",
    "                for action in self.action_space:\n",
    "                    self.Q[((pos_bin,vel_bin),action)] = 0\n",
    "                    self.returns[((pos_bin,vel_bin),action)] = 0\n",
    "                    self.visited[((pos_bin,vel_bin),action)] = 0\n",
    "                self.state_space.append((pos_bin,vel_bin))\n",
    "        \n",
    "        for state in self.state_space:\n",
    "            self.policy[state] = np.random.choice(self.action_space)\n",
    "                \n",
    "        \n",
    "        \n",
    "    def get_state(self,observation):\n",
    "        pos,vel = observation\n",
    "        pos_bin = np.digitize(pos,self.pos_space)\n",
    "        vel_bin = np.digitize(vel,self.vel_space)\n",
    "        \n",
    "        return pos_bin,vel_bin\n",
    "    \n",
    "    def max_action(self,state):\n",
    "        values = np.array([self.Q[state,a] for a in self.action_space])\n",
    "        action = np.argmax(values)\n",
    "        return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24eb296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "agent = agent()\n",
    "num_episodes = 100000\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "eps = 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf5d711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode  1000 score  -1000.0 eps  0.95999999999996\n",
      "starting episode  2000 score  -1000.0 eps  0.91999999999992\n",
      "starting episode  3000 score  -1000.0 eps  0.87999999999988\n",
      "starting episode  4000 score  -1000.0 eps  0.83999999999984\n",
      "starting episode  5000 score  -1000.0 eps  0.7999999999998\n",
      "starting episode  6000 score  -577.0 eps  0.75999999999976\n",
      "starting episode  7000 score  -406.0 eps  0.71999999999972\n",
      "starting episode  8000 score  -711.0 eps  0.67999999999968\n",
      "starting episode  9000 score  -1000.0 eps  0.63999999999964\n",
      "starting episode  10000 score  -462.0 eps  0.5999999999996\n",
      "starting episode  11000 score  -315.0 eps  0.55999999999956\n",
      "starting episode  12000 score  -248.0 eps  0.51999999999952\n",
      "starting episode  13000 score  -311.0 eps  0.47999999999950776\n",
      "starting episode  14000 score  -293.0 eps  0.4399999999995233\n",
      "starting episode  15000 score  -293.0 eps  0.3999999999995388\n",
      "starting episode  16000 score  -264.0 eps  0.3599999999995543\n",
      "starting episode  17000 score  -226.0 eps  0.3199999999995698\n",
      "starting episode  18000 score  -229.0 eps  0.2799999999995853\n",
      "starting episode  19000 score  -286.0 eps  0.23999999999959384\n",
      "starting episode  20000 score  -198.0 eps  0.1999999999995816\n",
      "starting episode  21000 score  -192.0 eps  0.15999999999956935\n",
      "starting episode  22000 score  -209.0 eps  0.11999999999955885\n",
      "starting episode  23000 score  -143.0 eps  0.07999999999956048\n",
      "starting episode  24000 score  -140.0 eps  0.03999999999956211\n",
      "starting episode  25000 score  -153.0 eps  0.01\n",
      "starting episode  26000 score  -154.0 eps  0.01\n",
      "starting episode  27000 score  -179.0 eps  0.01\n",
      "starting episode  28000 score  -220.0 eps  0.01\n",
      "starting episode  29000 score  -123.0 eps  0.01\n",
      "starting episode  30000 score  -125.0 eps  0.01\n",
      "starting episode  31000 score  -117.0 eps  0.01\n",
      "starting episode  32000 score  -146.0 eps  0.01\n",
      "starting episode  33000 score  -135.0 eps  0.01\n",
      "starting episode  34000 score  -142.0 eps  0.01\n",
      "starting episode  35000 score  -143.0 eps  0.01\n",
      "starting episode  36000 score  -143.0 eps  0.01\n",
      "starting episode  37000 score  -223.0 eps  0.01\n",
      "starting episode  38000 score  -127.0 eps  0.01\n",
      "starting episode  39000 score  -143.0 eps  0.01\n",
      "starting episode  40000 score  -142.0 eps  0.01\n",
      "starting episode  41000 score  -135.0 eps  0.01\n",
      "starting episode  42000 score  -137.0 eps  0.01\n",
      "starting episode  43000 score  -113.0 eps  0.01\n",
      "starting episode  44000 score  -214.0 eps  0.01\n",
      "starting episode  45000 score  -140.0 eps  0.01\n",
      "starting episode  46000 score  -139.0 eps  0.01\n",
      "starting episode  47000 score  -138.0 eps  0.01\n",
      "starting episode  48000 score  -138.0 eps  0.01\n",
      "starting episode  49000 score  -130.0 eps  0.01\n",
      "starting episode  50000 score  -136.0 eps  0.01\n",
      "starting episode  51000 score  -215.0 eps  0.01\n",
      "starting episode  52000 score  -135.0 eps  0.01\n",
      "starting episode  53000 score  -136.0 eps  0.01\n",
      "starting episode  54000 score  -164.0 eps  0.01\n",
      "starting episode  55000 score  -159.0 eps  0.01\n",
      "starting episode  56000 score  -134.0 eps  0.01\n",
      "starting episode  57000 score  -133.0 eps  0.01\n",
      "starting episode  58000 score  -140.0 eps  0.01\n",
      "starting episode  59000 score  -195.0 eps  0.01\n",
      "starting episode  60000 score  -143.0 eps  0.01\n",
      "starting episode  61000 score  -138.0 eps  0.01\n",
      "starting episode  62000 score  -144.0 eps  0.01\n",
      "starting episode  63000 score  -142.0 eps  0.01\n",
      "starting episode  64000 score  -162.0 eps  0.01\n",
      "starting episode  65000 score  -142.0 eps  0.01\n",
      "starting episode  66000 score  -132.0 eps  0.01\n",
      "starting episode  67000 score  -175.0 eps  0.01\n",
      "starting episode  68000 score  -132.0 eps  0.01\n",
      "starting episode  69000 score  -144.0 eps  0.01\n",
      "starting episode  70000 score  -126.0 eps  0.01\n",
      "starting episode  71000 score  -136.0 eps  0.01\n",
      "starting episode  72000 score  -139.0 eps  0.01\n",
      "starting episode  73000 score  -148.0 eps  0.01\n",
      "starting episode  74000 score  -203.0 eps  0.01\n",
      "starting episode  75000 score  -213.0 eps  0.01\n",
      "starting episode  76000 score  -131.0 eps  0.01\n",
      "starting episode  77000 score  -140.0 eps  0.01\n",
      "starting episode  78000 score  -302.0 eps  0.01\n",
      "starting episode  79000 score  -142.0 eps  0.01\n",
      "starting episode  80000 score  -138.0 eps  0.01\n",
      "starting episode  81000 score  -139.0 eps  0.01\n",
      "starting episode  82000 score  -137.0 eps  0.01\n",
      "starting episode  83000 score  -127.0 eps  0.01\n",
      "starting episode  84000 score  -152.0 eps  0.01\n",
      "starting episode  85000 score  -141.0 eps  0.01\n",
      "starting episode  86000 score  -142.0 eps  0.01\n",
      "starting episode  87000 score  -166.0 eps  0.01\n",
      "starting episode  88000 score  -117.0 eps  0.01\n",
      "starting episode  89000 score  -141.0 eps  0.01\n",
      "starting episode  90000 score  -142.0 eps  0.01\n",
      "starting episode  91000 score  -137.0 eps  0.01\n",
      "starting episode  92000 score  -139.0 eps  0.01\n",
      "starting episode  93000 score  -135.0 eps  0.01\n",
      "starting episode  94000 score  -140.0 eps  0.01\n",
      "starting episode  95000 score  -213.0 eps  0.01\n",
      "starting episode  96000 score  -144.0 eps  0.01\n",
      "starting episode  97000 score  -202.0 eps  0.01\n",
      "starting episode  98000 score  -147.0 eps  0.01\n",
      "starting episode  99000 score  -144.0 eps  0.01\n",
      "starting episode  100000 score  -139.0 eps  0.01\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 100000 is out of bounds for axis 0 with size 100000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ls/mdl2q7410zj32bxvvf078ff40000gn/T/ipykernel_76933/424453564.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                            -agent.Q[(state,action)])\n\u001b[1;32m     32\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_episodes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#     if eps - 1e-7 > 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 100000 is out of bounds for axis 0 with size 100000"
     ]
    }
   ],
   "source": [
    "rewards = np.zeros(num_episodes)\n",
    "#Algorithm starts\n",
    "for i in range(num_episodes):\n",
    "    done = False\n",
    "    \n",
    "    if i%1000 == 0 and i >0:\n",
    "        print(\"starting episode \",i,\"score \",score,\"eps \",eps)\n",
    "        file = open(\"rewards_q_learning\",'wb')\n",
    "        pickle.dump(rewards,file)\n",
    "        file.close()\n",
    "\n",
    "        # file = open(\"policy_montecarlo\",'wb')\n",
    "        # pickle.dump(agent.policy,file)\n",
    "        # file.close()\n",
    "\n",
    "        file = open(\"Q_q_learning\",'wb')\n",
    "        pickle.dump(agent.Q,file)\n",
    "        file.close()\n",
    "\n",
    "    obs = agent.env.reset()\n",
    "    state = agent.get_state(obs)\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.max_action(state) if np.random.random() > eps \\\n",
    "                                    else agent.env.action_space.sample()\n",
    "        obs_,reward,done,info = agent.env.step(action)\n",
    "        state_ = agent.get_state(obs_)\n",
    "        score += reward\n",
    "        action_ = agent.max_action(state_)\n",
    "        agent.Q[(state,action)] += alpha*(reward+gamma*agent.Q[(state_,action_)]\\\n",
    "                                           -agent.Q[(state,action)])\n",
    "        state = state_\n",
    "    rewards[i] = score\n",
    "    eps = eps-4/num_episodes if eps > 0.01 else 0.01\n",
    "#     if eps - 1e-7 > 0:\n",
    "#         eps -= 1e-7\n",
    "#     else:\n",
    "#         eps = 0\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c513b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
