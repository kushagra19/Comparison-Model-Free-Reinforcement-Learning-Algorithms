{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34cf314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9fccc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up agent and environment\n",
    "class agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"MountainCar-v0\")\n",
    "        self.env._max_episode_steps = 1000\n",
    "        self.pos_space = np.linspace(-1.2,0.6,20)\n",
    "        self.vel_space = np.linspace(-.07,.07,20)\n",
    "        self.action_space = [0,1,2]\n",
    "        self.state_space = []\n",
    "        self.Q = {}\n",
    "        self.returns = {}\n",
    "        self.visited = {}\n",
    "        self.policy = {}\n",
    "        \n",
    "        for pos_bin in range(len(self.pos_space)+1):\n",
    "            for vel_bin in range(len(self.vel_space)+1):\n",
    "                for action in self.action_space:\n",
    "                    self.Q[((pos_bin,vel_bin),action)] = 0\n",
    "                    self.returns[((pos_bin,vel_bin),action)] = 0\n",
    "                    self.visited[((pos_bin,vel_bin),action)] = 0\n",
    "                self.state_space.append((pos_bin,vel_bin))\n",
    "        \n",
    "        for state in self.state_space:\n",
    "            self.policy[state] = np.random.choice(self.action_space)\n",
    "                \n",
    "        \n",
    "        \n",
    "    def get_state(self,observation):\n",
    "        pos,vel = observation\n",
    "        pos_bin = np.digitize(pos,self.pos_space)\n",
    "        vel_bin = np.digitize(vel,self.vel_space)\n",
    "        \n",
    "        return pos_bin,vel_bin\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7258eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "agent = agent()\n",
    "gamma = 1\n",
    "eps = 1.00\n",
    "num_episode = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64601cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode  1000 score  -1000.0 eps  0.95999999999996\n",
      "starting episode  2000 score  -1000.0 eps  0.91999999999992\n",
      "starting episode  3000 score  -1000.0 eps  0.87999999999988\n",
      "starting episode  4000 score  -1000.0 eps  0.83999999999984\n",
      "starting episode  5000 score  -1000.0 eps  0.7999999999998\n",
      "starting episode  6000 score  -1000.0 eps  0.75999999999976\n",
      "starting episode  7000 score  -1000.0 eps  0.71999999999972\n",
      "starting episode  8000 score  -1000.0 eps  0.67999999999968\n",
      "starting episode  9000 score  -389.0 eps  0.63999999999964\n",
      "starting episode  10000 score  -284.0 eps  0.5999999999996\n",
      "starting episode  11000 score  -1000.0 eps  0.55999999999956\n",
      "starting episode  12000 score  -1000.0 eps  0.51999999999952\n",
      "starting episode  13000 score  -173.0 eps  0.47999999999950776\n",
      "starting episode  14000 score  -1000.0 eps  0.4399999999995233\n",
      "starting episode  15000 score  -162.0 eps  0.3999999999995388\n",
      "starting episode  16000 score  -249.0 eps  0.3599999999995543\n",
      "starting episode  17000 score  -228.0 eps  0.3199999999995698\n",
      "starting episode  18000 score  -186.0 eps  0.2799999999995853\n",
      "starting episode  19000 score  -162.0 eps  0.23999999999959384\n",
      "starting episode  20000 score  -162.0 eps  0.1999999999995816\n",
      "starting episode  21000 score  -152.0 eps  0.15999999999956935\n",
      "starting episode  22000 score  -154.0 eps  0.11999999999955885\n",
      "starting episode  23000 score  -152.0 eps  0.07999999999956048\n",
      "starting episode  24000 score  -160.0 eps  0.03999999999956211\n",
      "starting episode  25000 score  -154.0 eps  0.01\n",
      "starting episode  26000 score  -153.0 eps  0.01\n",
      "starting episode  27000 score  -152.0 eps  0.01\n",
      "starting episode  28000 score  -151.0 eps  0.01\n",
      "starting episode  29000 score  -151.0 eps  0.01\n",
      "starting episode  30000 score  -152.0 eps  0.01\n",
      "starting episode  31000 score  -157.0 eps  0.01\n",
      "starting episode  32000 score  -154.0 eps  0.01\n",
      "starting episode  33000 score  -155.0 eps  0.01\n",
      "starting episode  34000 score  -155.0 eps  0.01\n",
      "starting episode  35000 score  -153.0 eps  0.01\n",
      "starting episode  36000 score  -154.0 eps  0.01\n",
      "starting episode  37000 score  -156.0 eps  0.01\n",
      "starting episode  38000 score  -153.0 eps  0.01\n",
      "starting episode  39000 score  -152.0 eps  0.01\n",
      "starting episode  40000 score  -152.0 eps  0.01\n",
      "starting episode  41000 score  -153.0 eps  0.01\n",
      "starting episode  42000 score  -151.0 eps  0.01\n",
      "starting episode  43000 score  -150.0 eps  0.01\n",
      "starting episode  44000 score  -150.0 eps  0.01\n",
      "starting episode  45000 score  -156.0 eps  0.01\n",
      "starting episode  46000 score  -152.0 eps  0.01\n",
      "starting episode  47000 score  -153.0 eps  0.01\n",
      "starting episode  48000 score  -154.0 eps  0.01\n",
      "starting episode  49000 score  -154.0 eps  0.01\n",
      "starting episode  50000 score  -152.0 eps  0.01\n",
      "starting episode  51000 score  -152.0 eps  0.01\n",
      "starting episode  52000 score  -151.0 eps  0.01\n",
      "starting episode  53000 score  -152.0 eps  0.01\n",
      "starting episode  54000 score  -156.0 eps  0.01\n",
      "starting episode  55000 score  -151.0 eps  0.01\n",
      "starting episode  56000 score  -158.0 eps  0.01\n",
      "starting episode  57000 score  -152.0 eps  0.01\n",
      "starting episode  58000 score  -153.0 eps  0.01\n",
      "starting episode  59000 score  -155.0 eps  0.01\n",
      "starting episode  60000 score  -151.0 eps  0.01\n",
      "starting episode  61000 score  -150.0 eps  0.01\n",
      "starting episode  62000 score  -153.0 eps  0.01\n",
      "starting episode  63000 score  -154.0 eps  0.01\n",
      "starting episode  64000 score  -151.0 eps  0.01\n",
      "starting episode  65000 score  -153.0 eps  0.01\n",
      "starting episode  66000 score  -153.0 eps  0.01\n",
      "starting episode  67000 score  -153.0 eps  0.01\n",
      "starting episode  68000 score  -158.0 eps  0.01\n",
      "starting episode  69000 score  -155.0 eps  0.01\n",
      "starting episode  70000 score  -153.0 eps  0.01\n",
      "starting episode  71000 score  -151.0 eps  0.01\n",
      "starting episode  72000 score  -153.0 eps  0.01\n",
      "starting episode  73000 score  -153.0 eps  0.01\n",
      "starting episode  74000 score  -152.0 eps  0.01\n",
      "starting episode  75000 score  -153.0 eps  0.01\n",
      "starting episode  76000 score  -161.0 eps  0.01\n",
      "starting episode  77000 score  -150.0 eps  0.01\n",
      "starting episode  78000 score  -153.0 eps  0.01\n",
      "starting episode  79000 score  -154.0 eps  0.01\n",
      "starting episode  80000 score  -150.0 eps  0.01\n",
      "starting episode  81000 score  -152.0 eps  0.01\n",
      "starting episode  82000 score  -151.0 eps  0.01\n",
      "starting episode  83000 score  -155.0 eps  0.01\n",
      "starting episode  84000 score  -154.0 eps  0.01\n",
      "starting episode  85000 score  -153.0 eps  0.01\n",
      "starting episode  86000 score  -157.0 eps  0.01\n",
      "starting episode  87000 score  -151.0 eps  0.01\n",
      "starting episode  88000 score  -180.0 eps  0.01\n",
      "starting episode  89000 score  -153.0 eps  0.01\n",
      "starting episode  90000 score  -153.0 eps  0.01\n",
      "starting episode  91000 score  -156.0 eps  0.01\n",
      "starting episode  92000 score  -152.0 eps  0.01\n",
      "starting episode  93000 score  -151.0 eps  0.01\n",
      "starting episode  94000 score  -155.0 eps  0.01\n",
      "starting episode  95000 score  -153.0 eps  0.01\n",
      "starting episode  96000 score  -155.0 eps  0.01\n",
      "starting episode  97000 score  -153.0 eps  0.01\n",
      "starting episode  98000 score  -185.0 eps  0.01\n",
      "starting episode  99000 score  -152.0 eps  0.01\n"
     ]
    }
   ],
   "source": [
    "rewards = np.zeros(num_episode)\n",
    "#Algorithm starts\n",
    "for i in range(num_episode):\n",
    "    stateActionReturns = []\n",
    "    memory = []\n",
    "    if i%1000 == 0 and i>0:\n",
    "        print(\"starting episode \",i,\"score \",score,\"eps \",eps)\n",
    "        file = open(\"rewards_montecarlo\",'wb')\n",
    "        pickle.dump(rewards,file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"policy_montecarlo\",'wb')\n",
    "        pickle.dump(agent.policy,file)\n",
    "        file.close()\n",
    "\n",
    "        file = open(\"Q_montecarlo\",'wb')\n",
    "        pickle.dump(agent.Q,file)\n",
    "        file.close()\n",
    "    observation = agent.env.reset()\n",
    "    observation = agent.get_state(observation)\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.policy[observation]\n",
    "        observation_,reward,done,info = agent.env.step(action)\n",
    "        score += reward\n",
    "        observation_ = agent.get_state(observation_)\n",
    "        memory.append((observation[0],observation[1],action,reward))\n",
    "        observation = observation_\n",
    "    memory.append((observation[0],observation[1],action,reward))\n",
    "    rewards[i] = score\n",
    "    G = 0\n",
    "    last = True\n",
    "    for pos_bin,vel_bin,action,reward in reversed(memory):\n",
    "        if last:\n",
    "            last = False\n",
    "        else:\n",
    "            stateActionReturns.append((pos_bin,vel_bin,action,G))\n",
    "        G = gamma*G + reward\n",
    "    \n",
    "    stateActionReturns.reverse()\n",
    "    stateActionsVisited = []\n",
    "    \n",
    "    for pos_bin,vel_bin,action,G in stateActionReturns:\n",
    "        sa = ((pos_bin,vel_bin),action)\n",
    "        if sa not in stateActionsVisited:\n",
    "            agent.visited[sa] += 1\n",
    "            \n",
    "            agent.returns[sa] += (1/agent.visited[sa])*(G - agent.returns[sa])\n",
    "            agent.Q[sa] = agent.returns[sa]\n",
    "            rand = np.random.random()\n",
    "            if rand < 1-eps:\n",
    "                state = (pos_bin,vel_bin)\n",
    "                values = np.array([agent.Q[(state,a)] for a in agent.action_space])\n",
    "                best = np.random.choice(np.where(values == values.max())[0])\n",
    "                agent.policy[state] = agent.action_space[best]\n",
    "            else:\n",
    "                state = (pos_bin,vel_bin)\n",
    "                agent.policy[state] = np.random.choice(agent.action_space)\n",
    "            stateActionsVisited.append(sa)\n",
    "    eps = eps-4/num_episode if eps > 0.01 else 0.01\n",
    "#     if eps - 1e-7 > 0:\n",
    "#         eps -= 1e-7\n",
    "#     else:\n",
    "#         eps = 0\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d713f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Test policy\n",
    "\n",
    "# num_eps = 2000\n",
    "# rewards = np.zeros(num_eps)\n",
    "# totalReward = 0\n",
    "# for i in range(num_eps):\n",
    "#     observation = agent.env.reset()\n",
    "#     observation = agent.get_state(observation)\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action = agent.policy[observation]\n",
    "#         observation_,reward,done,info = agent.env.step(action)\n",
    "#         observation_ = agent.get_state(observation_)\n",
    "#         observation = observation_\n",
    "#         totalReward += reward\n",
    "#     rewards[i] = totalReward\n",
    "#     totalReward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db771806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261211b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078043b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
